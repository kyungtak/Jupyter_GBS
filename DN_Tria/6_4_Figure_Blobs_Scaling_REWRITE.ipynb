{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e16d467",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"font-size: 26px;\"><b>Blob analytical scaling law based on tracking (Script 6_0)</b></span><br>\n",
    "<span style=\"font-size: 16px;\">\n",
    "- In this script, we apply the analytical scaling laws derived in the past litteratures. <br>\n",
    "- There are two existing models for this:<br>\n",
    "- (1) One makes use of Lambda and Theta<br>\n",
    "- (2) The other makes use of hat-a and hat-v. \n",
    "</span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013f1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/marconi/home/userexternal/klim0000/my_venv/lib/python3.9/site-packages')\n",
    "#from skimage.measure import label, regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fc3169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('/marconi/home/userexternal/klim0000/miniconda3/lib/python3.9/site-packages/gbs_python/gbspy/blobs/Devel')\n",
    "sys.path.append('/marconi/home/userexternal/klim0000/my_venv/lib/python3.9/site-packages')\n",
    "sys.path.append('/marconi/home/userexternal/klim0000/miniconda3/lib/python3.9/site-packages/gbs_python/gbspy/blobs')\n",
    "from Blob_detection import *\n",
    "from CBlob_plots import *\n",
    "from Filament_utils import *\n",
    "from CBlobs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211ad7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gbspy as g\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2770ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gbspy.blobs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5982ccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /marconi/home/userexternal/klim0000/jupyter_notebook/3_GBS_DN_Tria\n"
     ]
    }
   ],
   "source": [
    "#import sys  \n",
    "#sys.path.insert(0, '/marconi/home/userexternal/klim0000/miniconda3/lib/python3.9/site-packages/gbs_python/gbspy')\n",
    "\n",
    "# Get the current working directory|\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# PT\n",
    "os.chdir('/marconi_scratch/userexternal/klim0000/GBS/DN_tria/delta_scan/DPT_d0p6')\n",
    "\n",
    "#NT\n",
    "#os.chdir('/marconi_scratch/userexternal/klim0000/GBS/DN_tria/delta_scan/DNT_dn0p6')\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Load GBS simulations\n",
    "s=g.Sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53c7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2= s.time[-1]\n",
    "t1= t2 - 0.1\n",
    "\n",
    "# Entire torus\n",
    "z1 = s.z[0]\n",
    "z2 = s.z[-1]\n",
    "iz = s.find_ind(z2, s.z)\n",
    "bsign = s.attribute[\"B0sign\"]\n",
    "\n",
    "\n",
    "dens         = s.get_field('n', z1=z1, z2=z2, t1=t1, t2=t2)\n",
    "dens_avg     = np.mean(dens, axis=(2,3)); \n",
    "dens_avg_tor = np.mean(dens, axis=2)\n",
    "dens_std     = np.std(dens, axis=(2,3))\n",
    "\n",
    "Te  = s.get_field('Te', z1=z1, z2=z2, t1=t1, t2=t2)\n",
    "Te_avg  = np.mean(Te, axis=(2,3)); \n",
    "Te_avg_tor = np.mean(Te, axis=2)\n",
    "\n",
    "pe = dens * Te\n",
    "pe_avg = np.mean(pe, axis=(2,3))\n",
    "pe_std = np.std(pe, axis=(2,3))\n",
    "\n",
    "Ti  = s.get_field('Ti', z1=z1, z2=z2, t1=t1, t2=t2)\n",
    "Ti_avg  = np.mean(Ti, axis=(2,3)); \n",
    "Ti_avg_tor = np.mean(Ti, axis=2)\n",
    "\n",
    "phi = s.get_field('strmf', z1=z1, z2=z2, t1=t1, t2=t2)\n",
    "phi_x = np.gradient(phi, axis=1)\n",
    "phi_y = np.gradient(phi, axis=0)\n",
    "\n",
    "\n",
    "# Normalized\n",
    "dens_fluct   = np.sqrt((dens[:,:,0,:]-dens_avg_tor)**2) / dens_avg_tor\n",
    "#dens_fluct = dens[:,:,0,-1] - dens_avg\n",
    "dens_fluct_avg = np.mean(dens_fluct, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8231a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"font-size: 26px;\"><b>Blob detection </b></span><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1e225ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "[79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.\n",
      " 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056. 79056.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(324, 244, 80, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(np.sum(mask, axis=(0, 1))))\n",
    "print(np.sum(mask, axis=(0, 1)))\n",
    "\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "262f91d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch_local/slurm_job.13028361/ipykernel_32974/2165753308.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  return (data - avg4D) / std4D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnflc = get_fluct_from_data(n_data, n_avg, n_std)\\nif hole == False:\\n    mask1[nflc[:, :, :, 0] < threshold] = 0\\nelse:\\n    mask1[nflc[:, :, :, 0] > threshold] = 0\\n\\n[NumObjects, PixelIdxList, TorIdxList] = get_connected_regions_torus(mask1, min_npx = len_tol)\\n\\n#Return from index in z compressed (less planes) to full (all planes in gbssim)\\nif len(z_new) < len(gbssim.z):\\n    for ifil in range(NumObjects):\\n        for ipix in range(len(PixelIdxList[ifil])):\\n            PixelIdxList[ifil][ipix] = (PixelIdxList[ifil][ipix][0],\\n                                        PixelIdxList[ifil][ipix][1],\\n                                        np.argmin(np.abs(gbssim.z\\n                                                         - z_new[PixelIdxList[ifil][ipix][2]])))\\nFilamentIndeces_OLD = np.zeros(NumObjects, dtype = int)\\nfor ib in range(NumObjects):\\n    FilamentList.append(Filament(gbssim, PixelIdxList[ib],\\n                                 TorIdxList[ib], its))\\n    FilamentIndeces_OLD[ib] = int(len(FilamentList) - 1)\\nNumObjects_OLD = NumObjects\\nPixelIdxList_OLD = PixelIdxList\\nprint(f\"Detection in step 1/{nsteps} done\")\\nfor it in range(its+1, ite+1):\\n    mask1 = np.array(mask[:, :, mask_z], copy=True)\\n    n_data = gbssim.get_field(which_field,\\n                              t1 = gbssim.time[it],\\n                              t2 = gbssim.time[it])[:, :, mask_z]\\n    nflc = get_fluct_from_data(n_data, n_avg, n_std)\\n    if hole == False:\\n        mask1[nflc[:, :, :, 0] < threshold] = 0\\n    else:\\n        mask1[nflc[:, :, :, 0] > threshold] = 0\\n    [NumObjects, PixelIdxList, TorIdxList] = get_connected_regions_torus(mask1,\\n                                                                         min_npx = len_tol)\\n    if len(z_new) < len(gbssim.z):\\n        for ifil in range(NumObjects):\\n            for ipix in range(len(PixelIdxList[ifil])):\\n                PixelIdxList[ifil][ipix] = (PixelIdxList[ifil][ipix][0],\\n                                            PixelIdxList[ifil][ipix][1],\\n                                            np.argmin(np.abs(gbssim.z\\n                                                             - z_new[PixelIdxList[ifil][ipix][2]])))\\n    FilamentIndeces = np.zeros(NumObjects, dtype = int)\\n    CheckOverlap = np.zeros((NumObjects_OLD, NumObjects), dtype = int)\\n    for ib_new in range(NumObjects):\\n        for ib_old in range(NumObjects_OLD):\\n            # To constrain the ordering of the triplets in PixelIdxList they are compared as strings\\n            Overlap = np.sum(np.isin([str(x) for x in PixelIdxList_OLD[ib_old]],\\n                                     [str(x) for x in PixelIdxList[ib_new]],\\n                                     assume_unique = True))\\n            if (Overlap > inter_tol * len(PixelIdxList_OLD[ib_old])):\\n                CheckOverlap[ib_old, ib_new] = 1\\n\\n    [_, ib_new_over] = np.nonzero(CheckOverlap)\\n\\n    alreadyadded = np.zeros(NumObjects, dtype = int)\\n    for iboverlap in ib_new_over:\\n        Sum_old = int(np.sum(CheckOverlap[:, iboverlap]))\\n        if (Sum_old == 1):\\n            ib_old = np.flatnonzero(CheckOverlap[:, iboverlap])\\n            ib_new = np.flatnonzero(CheckOverlap[ib_old, :])\\n            if (len(ib_new) == 1):\\n                if (not alreadyadded[ib_new]):\\n                    FilamentIndeces[ib_new[0]] = FilamentIndeces_OLD[ib_old[0]]\\n                    FilamentList[FilamentIndeces_OLD[ib_old[0]]].update(PixelIdxList[ib_new[0]],\\n                                                                        TorIdxList[ib_new[0]], it)\\n                    alreadyadded[ib_new] = 1\\n            else: #splitting\\n                for i in ib_new:\\n                    if (not alreadyadded[i]):\\n                        FilamentList.append(Filament(gbssim, PixelIdxList[i],\\n                                                     TorIdxList[i], it))\\n                        FilamentIndeces[i] = int(len(FilamentList) - 1)\\n                        alreadyadded[i] = 1\\n                        splitcount = splitcount +1\\n\\n        else: #merging\\n            if (not alreadyadded[iboverlap]):\\n                FilamentList.append(Filament(gbssim, PixelIdxList[iboverlap],\\n                                             TorIdxList[iboverlap], it))\\n                FilamentIndeces[iboverlap] = int(len(FilamentList) - 1)\\n                alreadyadded[iboverlap] = 1\\n                mergcount = mergcount + 1\\n    ind_blob_new = np.arange(NumObjects)\\n    ib_new = np.setdiff1d(ind_blob_new, ib_new_over)\\n    for ibnon_overlap in ind_blob_new: # new blobs\\n        if (not alreadyadded[ibnon_overlap]):\\n            FilamentList.append(Filament(gbssim,\\n                                         PixelIdxList[ibnon_overlap],\\n                                         TorIdxList[ibnon_overlap], it))\\n            FilamentIndeces[ibnon_overlap] = int(len(FilamentList) - 1)\\n            alreadyadded[ibnon_overlap] = 1\\n    NumObjects_OLD = NumObjects\\n    PixelIdxList_OLD = PixelIdxList\\n    FilamentIndeces_OLD = FilamentIndeces\\n    print(f\"Detection in step {it-its+1}/{nsteps} done\")\\nnfil_tot = len(FilamentList)\\nFilamentList = [fil for fil in FilamentList if len(fil.itime) > age_tol]\\nnfil_longtime = len(FilamentList)\\nprint(\\'{} merging events detected\\'.format(mergcount))\\nprint(\\'{} splitting events detected\\'.format(splitcount))\\nprint(\\'Detected {} filaments in total\\'.format(nfil_tot))\\nprint(\\'{} survived more than {} steps\\'.format(nfil_longtime, age_tol))\\nreturn FilamentList\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input\n",
    "\n",
    "mask = None\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "if mask is None:\n",
    "    mask = np.ones([s.ny, s.nx, s.nz])\n",
    "\n",
    "if any([t is None for t in (t1, t2)]):\n",
    "    its = 0\n",
    "    ite = len(s.time)-1\n",
    "else:\n",
    "    its = s.find_ind(s.time, t1) \n",
    "    ite = s.find_ind(s.time, t2) \n",
    "    \n",
    "nsteps = int(ite-its+1)\n",
    "\n",
    "\n",
    "\n",
    "[X, Y] = np.meshgrid(s.x, s.y)\n",
    "\n",
    "FilamentList = []\n",
    "splitcount = 0; mergcount = 0 \n",
    "\n",
    "mask_z = np.where(np.sum(mask, axis=(0, 1)) > 0)[0]\n",
    "# np.sum(mask, axis=(0, 1) = sum of all the elements in nx*ny\n",
    "mask1 = np.array(mask[:, :, mask_z], copy=True)\n",
    "z_new = s.z[mask_z]\n",
    "\n",
    "\n",
    "data = s.get_field('n', t1 = s.time[its], t2 = s.time[its])[:, :, mask_z] #4D\n",
    "data2 = np.mean(data, axis=2)\n",
    "data_avg = np.mean(data, axis=(2,3))\n",
    "data_std = np.std(data, axis=(2,3))\n",
    "\n",
    "fluct = get_fluct_from_data(data, data_avg, data_std)\n",
    "\n",
    "\n",
    "'''\n",
    "nflc = get_fluct_from_data(n_data, n_avg, n_std)\n",
    "if hole == False:\n",
    "    mask1[nflc[:, :, :, 0] < threshold] = 0\n",
    "else:\n",
    "    mask1[nflc[:, :, :, 0] > threshold] = 0\n",
    "\n",
    "[NumObjects, PixelIdxList, TorIdxList] = get_connected_regions_torus(mask1, min_npx = len_tol)\n",
    "\n",
    "#Return from index in z compressed (less planes) to full (all planes in gbssim)\n",
    "if len(z_new) < len(gbssim.z):\n",
    "    for ifil in range(NumObjects):\n",
    "        for ipix in range(len(PixelIdxList[ifil])):\n",
    "            PixelIdxList[ifil][ipix] = (PixelIdxList[ifil][ipix][0],\n",
    "                                        PixelIdxList[ifil][ipix][1],\n",
    "                                        np.argmin(np.abs(gbssim.z\n",
    "                                                         - z_new[PixelIdxList[ifil][ipix][2]])))\n",
    "FilamentIndeces_OLD = np.zeros(NumObjects, dtype = int)\n",
    "for ib in range(NumObjects):\n",
    "    FilamentList.append(Filament(gbssim, PixelIdxList[ib],\n",
    "                                 TorIdxList[ib], its))\n",
    "    FilamentIndeces_OLD[ib] = int(len(FilamentList) - 1)\n",
    "NumObjects_OLD = NumObjects\n",
    "PixelIdxList_OLD = PixelIdxList\n",
    "print(f\"Detection in step 1/{nsteps} done\")\n",
    "for it in range(its+1, ite+1):\n",
    "    mask1 = np.array(mask[:, :, mask_z], copy=True)\n",
    "    n_data = gbssim.get_field(which_field,\n",
    "                              t1 = gbssim.time[it],\n",
    "                              t2 = gbssim.time[it])[:, :, mask_z]\n",
    "    nflc = get_fluct_from_data(n_data, n_avg, n_std)\n",
    "    if hole == False:\n",
    "        mask1[nflc[:, :, :, 0] < threshold] = 0\n",
    "    else:\n",
    "        mask1[nflc[:, :, :, 0] > threshold] = 0\n",
    "    [NumObjects, PixelIdxList, TorIdxList] = get_connected_regions_torus(mask1,\n",
    "                                                                         min_npx = len_tol)\n",
    "    if len(z_new) < len(gbssim.z):\n",
    "        for ifil in range(NumObjects):\n",
    "            for ipix in range(len(PixelIdxList[ifil])):\n",
    "                PixelIdxList[ifil][ipix] = (PixelIdxList[ifil][ipix][0],\n",
    "                                            PixelIdxList[ifil][ipix][1],\n",
    "                                            np.argmin(np.abs(gbssim.z\n",
    "                                                             - z_new[PixelIdxList[ifil][ipix][2]])))\n",
    "    FilamentIndeces = np.zeros(NumObjects, dtype = int)\n",
    "    CheckOverlap = np.zeros((NumObjects_OLD, NumObjects), dtype = int)\n",
    "    for ib_new in range(NumObjects):\n",
    "        for ib_old in range(NumObjects_OLD):\n",
    "            # To constrain the ordering of the triplets in PixelIdxList they are compared as strings\n",
    "            Overlap = np.sum(np.isin([str(x) for x in PixelIdxList_OLD[ib_old]],\n",
    "                                     [str(x) for x in PixelIdxList[ib_new]],\n",
    "                                     assume_unique = True))\n",
    "            if (Overlap > inter_tol * len(PixelIdxList_OLD[ib_old])):\n",
    "                CheckOverlap[ib_old, ib_new] = 1\n",
    "\n",
    "    [_, ib_new_over] = np.nonzero(CheckOverlap)\n",
    "\n",
    "    alreadyadded = np.zeros(NumObjects, dtype = int)\n",
    "    for iboverlap in ib_new_over:\n",
    "        Sum_old = int(np.sum(CheckOverlap[:, iboverlap]))\n",
    "        if (Sum_old == 1):\n",
    "            ib_old = np.flatnonzero(CheckOverlap[:, iboverlap])\n",
    "            ib_new = np.flatnonzero(CheckOverlap[ib_old, :])\n",
    "            if (len(ib_new) == 1):\n",
    "                if (not alreadyadded[ib_new]):\n",
    "                    FilamentIndeces[ib_new[0]] = FilamentIndeces_OLD[ib_old[0]]\n",
    "                    FilamentList[FilamentIndeces_OLD[ib_old[0]]].update(PixelIdxList[ib_new[0]],\n",
    "                                                                        TorIdxList[ib_new[0]], it)\n",
    "                    alreadyadded[ib_new] = 1\n",
    "            else: #splitting\n",
    "                for i in ib_new:\n",
    "                    if (not alreadyadded[i]):\n",
    "                        FilamentList.append(Filament(gbssim, PixelIdxList[i],\n",
    "                                                     TorIdxList[i], it))\n",
    "                        FilamentIndeces[i] = int(len(FilamentList) - 1)\n",
    "                        alreadyadded[i] = 1\n",
    "                        splitcount = splitcount +1\n",
    "\n",
    "        else: #merging\n",
    "            if (not alreadyadded[iboverlap]):\n",
    "                FilamentList.append(Filament(gbssim, PixelIdxList[iboverlap],\n",
    "                                             TorIdxList[iboverlap], it))\n",
    "                FilamentIndeces[iboverlap] = int(len(FilamentList) - 1)\n",
    "                alreadyadded[iboverlap] = 1\n",
    "                mergcount = mergcount + 1\n",
    "    ind_blob_new = np.arange(NumObjects)\n",
    "    ib_new = np.setdiff1d(ind_blob_new, ib_new_over)\n",
    "    for ibnon_overlap in ind_blob_new: # new blobs\n",
    "        if (not alreadyadded[ibnon_overlap]):\n",
    "            FilamentList.append(Filament(gbssim,\n",
    "                                         PixelIdxList[ibnon_overlap],\n",
    "                                         TorIdxList[ibnon_overlap], it))\n",
    "            FilamentIndeces[ibnon_overlap] = int(len(FilamentList) - 1)\n",
    "            alreadyadded[ibnon_overlap] = 1\n",
    "    NumObjects_OLD = NumObjects\n",
    "    PixelIdxList_OLD = PixelIdxList\n",
    "    FilamentIndeces_OLD = FilamentIndeces\n",
    "    print(f\"Detection in step {it-its+1}/{nsteps} done\")\n",
    "nfil_tot = len(FilamentList)\n",
    "FilamentList = [fil for fil in FilamentList if len(fil.itime) > age_tol]\n",
    "nfil_longtime = len(FilamentList)\n",
    "print('{} merging events detected'.format(mergcount))\n",
    "print('{} splitting events detected'.format(splitcount))\n",
    "print('Detected {} filaments in total'.format(nfil_tot))\n",
    "print('{} survived more than {} steps'.format(nfil_longtime, age_tol))\n",
    "return FilamentList\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "595b8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fluct_from_data(data, avg, std):\n",
    "    \"\"\" \n",
    "    Return fluctuation values from data as (data-avg)/std\n",
    "    where avg and std have lower or equal dimension than data\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 4:\n",
    "        avg4D = np.repeat(np.repeat(avg[:, :, np.newaxis], \n",
    "                                    data.shape[2], axis = 2)[:, :, :, np.newaxis], \n",
    "                          data.shape[3], axis = 3)\n",
    "        std4D = np.repeat(np.repeat(std[:, :, np.newaxis], \n",
    "                                    data.shape[2], axis = 2)[:, :, :, np.newaxis], \n",
    "                          data.shape[3], axis = 3)\n",
    "        return (data - avg4D) / std4D\n",
    "    elif len(data.shape) == 3:\n",
    "        avg3D = np.repeat(avg[:, :, np.newaxis], \n",
    "                                    data.shape[2], axis = 2)\n",
    "        std3D = np.repeat(std[:, :, np.newaxis], \n",
    "                                    data.shape[2], axis = 2)\n",
    "        return (data - avg3D) / std3D\n",
    "    elif len(data.shape) == 2:\n",
    "        return (data - avg) / std \n",
    "    else:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddd39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
